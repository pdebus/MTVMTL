\begin{chapter}{Theory}
\label{ch:theory}

\section{Riemannian Newton Method} % (fold)
\label{sec:Riemannian Newton Method}

% section Riemannian Newton Method (end)

\section{Algorithms} % (fold)
\label{sec:Algorithms}

\subsection{IRLS} % (fold)
\label{sub:IRLS}

% subsection IRLS (end)

\subsection{Proximal Point} % (fold)
\label{sub:Proximal Point}

% subsection Proximal Point (end)

% section Algorithms (end)

\section{Manifolds} % (fold)
\label{sec:Manifolds}

\subsection{Euclidian} % (fold)
\label{sub:Euclidian}

% subsection Euclidian (end)

\subsection{Sphere $S^n$} % (fold)
\label{sub:Sphere}

% subsection Sphere (end)

\subsection{Special Orthogonal Group SO(n)} % (fold)
\label{sub:SO(N)}

\subsubsection{Second derivatives of the distance function} % (fold)
\label{ssub:Second derivatives of the distance function}
For the computation of the second derivatives we can take the expression obtained using the above theorem as a starting point and follow the approach and notation of Magnus \cite{magnus}. 
This allows us to express the derivatives as combinations of simple Kronecker product of the arguments which also is very straightforward and compact to implement. 
The detailed derivations can be found in the appendix \ref{appendix} while here we only represent the final results. \\
For the second derivative with respect to the first argument one readily arrives at
\begin{equation}
    \label{eq:son_xx_der}
    \frac{\partial^2 d^2(X,Y)}{\partial X\partial X} = -2\left[\left(\left(\log X^TY\right)^T\otimes\mathbbm{1}_n\right) + \left(\mathbbm{1}_n\otimes X \right)\operatorname{D}\log(X^TY) \left(Y^T\otimes\mathbbm{1}_n \right) K_{nn}\right],
\end{equation}
where $K_{nn}$ denotes the commutator matrix which transforms the columnwise vectorization of a matrix $A$ to the vectorization of its tranpose $A^T$.\\

The mixed derivative is given by
\begin{equation}
    \label{eq:son_xy_der}
    \frac{\partial^2 d^2(X,Y)}{\partial X\partial Y} = -2\left(\mathbbm{1}_n\otimes X \right)\operatorname{D}\log(X^TY) \left(\mathbbm{1}_n\otimes X^T \right),
\end{equation}
These expressions are quasi-analytical: Matrix logarithms, exponentials and the Frechet derivative of the logarithms need to be evaluated numerically. Details concerning the 
implementation of the latter are postponed to section \ref{seq:frechetderivatives}.

% subsubsection Second derivatives of the distance function (end)


% subsection SO(N) (end)

\subsection{Symmetric Positive Definite Matrices SPD(n)} % (fold)
\label{sub:SPD(N)}

\subsubsection{Second derivatives of the distance function} % (fold)
\label{ssub:Second derivatives of the distance function}
For the SPD matrices we proceed in the same way as for the orthogonal group and obtain
\begin{align}
    \label{eq:spd_xx_der}
    \frac{\partial^2 d^2(X,Y)}{\partial X\partial X} &= 
    2\Bigg[
	\left(X^{-\frac{1}{2}}\log\left(X^{-\frac{1}{2}}YX^{-\frac{1}{2}}\right)^T\otimes\mathbbm{1}_n\right)
	+\left(\mathbbm{1}_n\otimes X^{-\frac{1}{2}}\log\left(X^{-\frac{1}{2}}YX^{-\frac{1}{2}}\right)\right) \\
    &	+\left(X^{-\frac{1}{2}}\otimes X^{-\frac{1}{2}}\right)\operatorname{D}\log(X^{-\frac{1}{2}}YX^{-\frac{1}{2}})\left( \left(X^{-\frac{1}{2}}Y\otimes\mathbbm{1}_n\right)
	+ \left(\mathbbm{1}_n\otimes X^{-\frac{1}{2}}Y\right)\right) 
    \Bigg] \times\cdots \nonumber \\
    & \cdots\times\left(X^{-\frac{1}{2}}\otimes X^{-\frac{1}{2}}\right)\operatorname{D}(X^{\frac{1}{2}})\nonumber
\end{align}
and for the mixed derivatives
\begin{equation}
    \label{eq:spd_xy_der}
    \frac{\partial^2 d^2(X,Y)}{\partial X\partial Y} = -2\left(X^{-\frac{1}{2}}\otimes X^{-\frac{1}{2}}\right)\operatorname{D}\log\left(X^{-\frac{1}{2}}Y X^{-\frac{1}{2}}\right)\left(X^{-\frac{1}{2}}\otimes X^{-\frac{1}{2}}\right)
\end{equation}

% subsubsection Second derivatives of the distance function (end)

% subsection SPD(N) (end)

\subsection{Grassmanian Gr(n,p)} % (fold)
\label{sub:Grassmanian}
The Grassmann manifold is special among the manifolds so far considered due to the fact that it is a quotient manifold. 
As such, there are different possbilieties for choosing equivalence classes and representatives thereof from some matrix space that need to be addressed
before an implementation.\\

For positive integers $n$ and $p\leq n$ the Grassmann manifold is defined as the set of $p$-dimensional linear subspaces of $\mathbb{R}^n$. 
Since a linear subspace $\mathcal{Y}\in Gr(n,p)$ can be specified using a basis, we can arrange its basis vectors as columns of
a matrix $Y\in\mathbb{R^{n\times p}}$ such that its column space spans $\mathcal{Y}$. The rank of $Y$ must necessarily be full and equal to $p$ because of the linear independence
of its columns. Hence, elements of $Gr(n,p)$ can be represented using elements of the \emph{non-compact Stiefel manifold}
\begin{equation}
    \tilde St(n,p) := \left\lbrace Y\in\mathbb{R}^{n\times p}:\; \operatorname{rank}Y=p\right\rbrace.
\end{equation}

\subsubsection{Quotient representations} % (fold)
\label{ssub:Quotient representations}
Observing now that post-multiplication by any invertible $G\in Gl(p)$ does not change the span of $Y$, we can form the equivalence classes
\begin{equation}
    Y\,GL(p) := \left\lbrace YG: G\in Gl(p)\right\rbrace
\end{equation}
consisting of all matrices having the same span as $Y$. These equivalence classes can be thought of as the distinct elements of the Grassmannian which
leads to the following quotient manifold representation.\\
\begin{equation}
    Gr(n,p):=\tilde St(n,p) / Gl(p)
\end{equation}

This representation used by Absil et al\cite{AbsilGrasmman} which is very general because only the rank is specified. In the next steps of presenting the relavant quantities for the
algorithm we will follow Absil's derivation and notation but choose the quotient representation used by Edelman et al \cite{EAS} which is based on the orthogonal group. This will simplify the
most expressions and is also desirable from an algorithmic point of view as it removes more degrees of freedom in the choice of possbily unique representatives.\\

For the sake of completeness we also mention a completely different approach by Sato and Iwai \cite{SatoIwai} who choose $\mathbb{R}^{n\times n}$ as embedding space
where elements of $Gr(n,p)$ are given by rank $p$ orthogonal projection matrices. The presented application are, however, mostly eigenvalue problems while in the case
of image denoising the increased memory requirements are disadvantageous.\\

The orthogonal group quotient represenation of the Grassmann manifold is given by
\begin{equation}
    Gr(n,p) = St(n,p) / O(p) %= O(n) / (O(p) \times O(n-p)),
\end{equation}
where we denote by $St(n,p)=\left\lbrace Y\in\mathbb{R}^{n\times p}:\; Y^TY=\mathbbm{1}_p,\right\rbrace$ the \emph{compact} Stiefel manifold with the additional
requirement that the basis spanning the subspace $\mathcal{Y}$ be orthonormal now. The canoncial quotient projection map is then given by
\begin{equation}
    \pi : St(n,p)\ni Y \mapsto \operatorname{span} Y=\mathcal{Y} \in Gr(n, p)
\end{equation}
% subsubsection Quotient representations (end)

\subsubsection{Locally unique representatives} % (fold)
\label{ssub:Locally unique Representative}
Let $U\in St(n,p)$ and define the local affine cross section through $U$ and orthogonal to the fiber $U[O(p)]=\pi^{-1}(\mathcal{U})\subset St(n,p)$ by
\begin{equation}
    S_U := \left\lbrace V\in St(n,p): U^T(V-U)=0 \right\rbrace\subset St(n,p).
\end{equation}

The equivalence class of $V\in St(n,p)$ is equal to $\pi^{-1}(\pi(V))=V\,O(p)$ and to calculate its intersection with $S_U$ we choose $R\in O(p)$ such that
$VR\in VO(p)$ and obtain
\begin{align}
    VR\in S_U\;\Leftrightarrow\; U^T(VR-U)=0 \;\Leftrightarrow\; R = (U^TV)^{-1}
\end{align}
which leads to the intersection
\begin{align}
    S_U \cap V[O(p)] = \left\lbrace VR = V(U^{T}V)^{-1} \right\rbrace .
\end{align}
which can be also empty if $U^{T}S$ is not invertible. Finally, we define a \emph{cross-section mapping} $\sigma_U$ restricted to the set
\begin{equation}
    \mathcal{U}_U := \left\lbrace\mathcal{V}=\operatorname{span}V:\; U^TV\in GL(p) \right\rbrace
\end{equation}
by 
\begin{equation}
    \sigma_U: Gr(n,p)\supset \mathcal{U}_U\ni\mathcal{V}=\operatorname{span}V\mapsto V(U^{T}V)^{-1} \in S_U \subset St(n,p).
\end{equation}
which is a diffeomorphism providing the differentiable structure considering the embedding of $St(n,p)$ in Euclidian space.\\

The above considerations are important for the design of algorithms for two reasons.\\
Firstly, it provides the means to give well-defined expressions for various quantities we want to compute using arbitrary representatives.
For the case of an average for instance, we can take representatives $Y_1,\ldots,Y_n\in St(n,p)$ for $\mathcal{Y}_1,\ldots,\mathcal{Y}_n\in Gr(n,p)$
and find a $U\in St(n,p)$ such that $S_U$ has non-zero intersection with all the $Y_i$'s equivalence classes, which is equivalent
to $U^TY_i\in Gl(p)$. The average $\mathcal{A}$ can then be written as
\begin{equation}
    \mathcal{A} := \pi\left(\sum_{i=1}^{n}\sigma_U(Y_i)\right)=\pi\left(\sum_{i=1}^{n}Y_i(U^{T}Y_i)^{-1}\right).
\end{equation}

Secondly, it allows us to find a parametrization of $Gr(n,p)$ in terms of $\mathbb{R}^{n\times p}$ matrices. This is necessary to construct a local basis of the tangent base
and make the dimension of the sparse linear system a function of the intrinsic manifold dimension $(n-p)p$ instead of the embedding dimension $np$.
% subsubsection Locally unique Representative (end)

\subsubsection{Tangent space} % (fold)
\label{ssub:Tangent space}
Due to the quotient structure which forces us to work with representatives we cannot just use the usual method for finding the tangent space by differentiating
curves on the manifold but have to start with "numerator" of the quotient $St(n,p)$ instead. For the Grassmann manifolds only tangent vectors of a special subspace of $T_YSt(n,p)$,
the horizontal space, can modify the span of subspace and exactly those belong to the tangent space of $Gr(n,p)$. \\

Let $Y\in St(n,p)\subset\mathbb{R}^{n\times p}$. Then tangent space at $Y$ (\cite{AbsilOptim} for details of the derivation) to the compact Stiefel manifold is given by
\begin{align}
    \label{eq:stiefel_tangentspace}
   T_YSt(n,p)	&= \left\lbrace Z\in\mathbb{R}^{n\times p}: Y^TZ+Z^TY=0 \right\rbrace\\
   &=  \left\lbrace Y\Omega + Y_{\bot}K: \Omega\in\operatorname{Skew}(p),\, K\in\mathbb{R}^{(n-p)\times p} \right\rbrace\nonumber
\end{align}
where $Y_{\bot}\in\mathbb{R}^{(n\times (n-p)}$ is defined such that $[Y,Y_{\bot}]\in O(n)$. The second representation of (\ref{stiefel_tangentspace})
already implies the decomposition into vertical and horizontal spaces we are going to perform next.

The vertical space at $Y$ is by definition the tangent space to the fiber $\pi^{-1}(\pi(Y))$
\begin{equation}
    \label{eq:stiefel_horizontalspace}
    V_Y = T_Y\pi^{-1}(\pi(Y))=T_YY[O(p)]=Y[\operatorname{Skew}(p)],
\end{equation}
while the horizontal space is defined as its orthogonal complement with respect to (\ref{eq:stiefel_tangentspace})
\begin{equation}
    \label{eq:stiefel_verticalspace}
    H_Y=V_Y^{\bot} =\left\lbrace H\in T_Y St(n,p):Y^TH=0 \right\rbrace \simeq Y_{\bot}[\mathbb{R}^{(n-p)\times p}].
\end{equation}

Using this, the tangent space to $Gr(n,p)$ at $\pi(Y)=\mathcal{Y}$, along with its projector is given by 
\begin{align}
    \label{eq:grassmann_tangentspace}
    T_{\mathcal{Y}}Gr(n,p)&\simeq  H_YSt(n,p)\simeq Y_{\bot}[\mathbb{R}^{(n-p)\times p}]\\
    \pi_{Y_{\bot}}&:=\mathbbm{1}_n-YY^T
\end{align}


% subsubsection Tangent space (end)

\subsubsection{Exponential map} % (fold)
\label{ssub:Exponential map}
Let $X, Y$ span $\mathcal{X}, \mathcal{Y}$, respectively and let $U\Sigma V^{T}$ denote the compact singular value decomposition of $Y$. Then
\begin{equation}
    \operatorname{Exp}_{\mathcal{X}}(\mathcal{Y})=\operatorname{span}\left( XV\cos\Sigma + U\sin\Sigma\right).
\end{equation}
% subsubsection Exponential map (end)



\subsubsection{Distance function} % (fold)
\label{ssub:Distance function}
Using the previously defined exponential map, one can easily define a geodesic distance function on the Grassmann manifold which is induced by its Riemannian metric
\begin{equation}
    g(X,Y) = \tr X^TY
\end{equation}
Then the distance function is given by the principal angles $\theta_i$ between the subspaces 
\begin{equation}
    \label{eq:gr_geod_dist}
    d_g^2(X,Y) = \norm{\theta}_2^2=\sum_{i=1}^p\theta_i^2,
\end{equation}
where the the principal angles can be obtained by computing the singular value decomposition of $X^TY$.

\begin{align}
    X^TY &= U\Sigma V^T = U\cos\Theta V^T\\
    \Sigma &= \operatorname{diag} (\sigma_1,\cdots,\sigma_p)\\
    \Theta &= \operatorname{diag} (\theta_1,\cdots,\theta_p) = \operatorname{diag} (\arccos\sigma_1,\cdots,\arccos\sigma_p)
\end{align}

The distance function (\ref{eq:gr_geod_dist}) has the disadvantage that due to the occurence of the cosine no analytic derivatives can be computed.\\

To avoid this problem, we follow Absil's \cite{absil_GR} approach and choose an equivalent norm, the so-called projection Frobenius norm, given by

\begin{equation}
    d_P^2(X,Y) = \frac{1}{2}\norm{XX^T-YY^T}_{F}^{2} = \sum_{i=1}^p\sin^2\theta_i
\end{equation}


% subsubsection Distance function (end)


\subsubsection{First derivatives of the distance function} % (fold)
\label{ssub:First derivatives of the distance function}

\begin{equation}
    \label{eq:gr_x_der}
    \frac{\partial d^2(X,Y)}{\partial X} = 2\left(XX^T-YY^T\right)X
\end{equation}


% subsubsection First derivatives of the distance function (end)

\subsubsection{Second derivatives of the distance function} % (fold)
\label{ssub:Second derivatives of the distance function}

\begin{equation}
    \label{eq:gr_xx_der}
    \frac{\partial^2 d^2(X,Y)}{\partial X\partial X} = 2
    \left[
	\left(X^TX\otimes \mathbbm{1}_{n}\right) 
	+ \left(\mathbbm{1}_p\otimes (XX^T-YY^T)\right)
	+ \left(X^T\otimes X\right)K_{np}
    \right]
\end{equation}

The mixed derivative is given by
\begin{equation}
    \label{eq:gr_xy_der}
    \frac{\partial^2 d^2(X,Y)}{\partial X\partial Y} = -2\left[\left(X^TY\otimes \mathbbm{1}_{n}\right) + \left(X^T\otimes Y\right)K_{np}\right]
\end{equation}


% subsubsection Second derivatives of the distance function (end)


% subsection Grassmanian Gr(N,P) (end)

% section Manifolds (end)

\section{Fr\'{e}chet derivatives of matrix logarithm and square root} % (fold)
\label{sec:frechetderivatives}
To use the derivative expression computed above we need the so called Kronecker form of the Fr\'{e}chet derivative. The Fr\'{e}chet derivative
of a matrix valued function $f:\mathbb{R}^{n\times n}\to\mathbb{R}^{n\times n}$ at a point $X\in\mathbb{R}^{n\times n}$ is a linear function mapping $E\in\mathbb{R}^{n\times n}$
to $L_f(X,E)\in \mathbb{R}^{n\times n}$ such that
\begin{equation}
    f(X+E) - f(X) - L_f(X,E) = \mathcal{o}(\norm{E}).
\end{equation}
Chain rule and inverse function theorem also hold for the Fr\'{e}chet derivative:
\begin{align}
    L_{f\circ g}(X, E) &= L_{f}(g(X)),L_{g}(X,E))\\
    L_{f}(X, L_{f^{-1}}(f(X),E)) &= E
\end{align}

As we did in our formulation of the derivatives of the distance function, it can also be represented in the Kronecker form in which is represented as map
$K_f:\mathbb{R}^{n^2}\to\mathbb{R}^{n^2}$, such that $K_f(X)\in \mathbb{R}^{n^2\times n^2}$ is defined by
\begin{equation}
    \label{eq:kronckerform}
    \operatorname{vec}(L_f(X,E))=K_f(X)\operatorname{vec}(E)
\end{equation}
where $\operatorname{vec}:\mathbb{R}^{n\times n}\to\mathbb{R}^{n^2}$ denotes the columnwise vectorization operator.

\subsection{Derivative of the matrix square root} % (fold)
\label{sub:Derivative of the matrix square root}
We start by considering the Fr\'{e}chet derivative of $f(X)=X^2$, which is given by
\begin{equation}
    L_{X^2}(X,E) = XE + EX.
\end{equation}
Applying the inverse function theorem consequently leads to
\begin{equation}
    L_{X^2}(X^{\frac{1}{2}},L_{X^{\frac{1}{2}}}(X,E))=X^{\frac{1}{2}}L_{X^{\frac{1}{2}}}(X,E) + L_{X^{\frac{1}{2}}}(X,E)X^{\frac{1}{2}} = E,
\end{equation}
where the last equality shows that the Fr\'{e}chet derivative of the matrix square root $L_{X^{\frac{1}{2}}}(X,E)$ satisfies the Sylvester equation 
\begin{equation}
    \label{eq:sylvester}
    X^{\frac{1}{2}}L + LX^{\frac{1}{2}} = E,\quad L:=L_{X^{\frac{1}{2}}}(X,E).
\end{equation}

The Kronecker representation $K_{X^{\frac{1}{2}}}$ can now be obtained be using the vectorization operator on both sides of the equation and rearrange the term
to the form (\ref{eq:kronckerform}) which leads to
\begin{equation}
    K_{X^{\frac{1}{2}}}(X)=\left[\left(\mathbbm{1}\otimes X^{\frac{1}{2}}\right)+\left(X^{\frac{1}{2}T}\otimes \mathbbm{1}\right) \right]^{-1}.
\end{equation}
However, this straightforward approach has the disadvantage that the inverse of a $n^2\times n^2$ matrix need to be computed which has complexity
$\mathcal{O}((n^2)^3)=\mathcal{O}(n^6)$. In addition to that, the inverse needs to be found explicitly which is not numerically stable in general.\\

The Sylvester equation (\ref{eq:sylvester}), on the other hand, can be solved with $\mathcal{O}(n^3)$ operations via Schur transformation. We choose $E^{ij}$, the single-entry
matrices having 1 at $(i,j)$ and zero everywhere else, as a basis for $\mathbb{R}^{n\times n}$ and solve the Sylvester equation for each of the $n^2$ 
basis matrix elements. By that, the total complexity can be reduced to $n^2\mathcal{O}(n^3)=\mathcal{O}(n^5)$ and we avoid the 
potentially problematic explicit computation of inverses altogether.\\

We then obtain the final Kronecker form of the derivative by constructing its rows from the vectorized, tranposed Fr\'{e}chet derivatives:
\begin{equation}
    \left(K_{X^{\frac{1}{2}}}\right)_{in + j,\cdot} = \operatorname{vec}\left(L_{X^{\frac{1}{2}}}(X,E^{ij})^T\right) 
\end{equation}

\subsection{Derivative of the matrix logarithm} % (fold)
\label{sub:Derivative of the matrix logarithm}
For the logarithm we follow the approach described by Al-Mohy et al \cite{almohy_frechetlog} which is based on the differentiation of the Pad\'{e} approximant to $\log(1+X)$.
Since this is only applicable if the norm of $X$ is sufficiently small, the  use of an inverse scaling and squaring technique based on the relation
\begin{equation}
    \log(X) = 2\log(X^{\frac{1}{2}})
\end{equation}
is necessary.\\

Application of the chain rule leads to
\begin{equation}
    L_{\log}(X,E_0) = 2\log\left(X^{\frac{1}{2}},L_{X^{\frac{1}{2}}}(X,E_0)\right).
\end{equation}
The second argument on the right hand side can again be written as solution $E_1:=L_{X^{\frac{1}{2}}}(A,E_0)$ of an Sylvester-type equation
\begin{equation}
    X^{\frac{1}{2}}E_1+E_{1}X^{\frac{1}{2}}=E_0.
\end{equation}
Repeating the procedure $s$ times results in
\begin{align}
    L_{\log}(X,E_0)&=2^sL_{\log}\left(X^{\frac{1}{2^s}},E_s\right)\\
    X^{\frac{1}{2^{i}}}E_i+E_iX^{\frac{1}{2^{i}}}&=E_{i-1},\quad i=1,\ldots,s
\end{align}
where $E_s$ is obtained by successivly solving the set of Sylvester equations defined in the second line.\\

Finally, the Pad\'{e} approximant of order $m$ in its partial fraction form \cite{highham_pade} is given by
\begin{equation}
    \label{eq:pade_log}
    r_m(X) = \sum_{j=1}^{m}\alpha_j^{(m)}(\mathbbm{1}+\beta_j^{(m)}X)^{-1}X
\end{equation}
where $\alpha_{j}^{(m)},\beta_{j}^{(m)}\in (0,1)$ are the $m$-point Gauss-Legendre quadrature weights and nodes.\\

The derivative of (\ref{eq:pade_log}) is then easily computed as 
\begin{equation}
    L_{r_m}(X,E) = \sum_{j=1}^m\alpha_j^{(m)}(\mathbbm{1}+\beta_j^{(m)}X)^{-1}E(\mathbbm{1}+\beta_j^{(m)}X)^{-1}
\end{equation}
which leads to the final approximation of the matrix logarithm derivative, 
\begin{equation}
    \label{eq:dlog_approx}
    L_{\log}(X,E)\approx 2^{s}L_{r_m}\left(X^{\frac{1}{2^s}}-\mathbbm{1},E_s \right).
\end{equation}

For the implementation of (\ref{eq:dlog_approx}) we use algorithm 5.1 from \cite{almohy_frechetlog} with fixed $m=7$.
The Kronecker representation is then constructed as in the square root case.
% subsection Derivative of the matrix logarithm (end)




% subsection Derivative of the matrix square root (end)


% section Frechet derivative of the matrix logarithm and square root (end)


\end{chapter}
